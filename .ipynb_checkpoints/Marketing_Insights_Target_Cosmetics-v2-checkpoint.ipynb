{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ggXLdNuAGoi"
   },
   "source": [
    "## Today you are a Machine Learning Engineer at the Department of Business Intelligence at Target Cosmetics!\n",
    "This work relies on processed data from Kaggle https://www.kaggle.com/mkechinov/ecommerce-events-history-in-cosmetics-shop\n",
    "\n",
    "This work is motivated by the publication https://arxiv.org/pdf/2102.01625.pdf\n",
    "\n",
    "Further details available at\n",
    "https://arxiv.org/pdf/2010.02503.pdf\n",
    "\n",
    "You have access to the Target server data, specifically the Cosmetics section, such that you have NO customer facing information, but, only access to timestamped data regarding product viewing/carting/purchasing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKteRz3TjiAZ"
   },
   "source": [
    "### The user-journey data regarding user-product interaction is given to you as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9193KiGAGol"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='image10.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfN54fgUAGop"
   },
   "source": [
    "## The data above has been wrangled and converted to one-hot encoded format for this exercise.\n",
    "## You have access to first 250,000 user-journeys only and you have been asked to create a proof of concept analysis based on this data. Your task is to perform the following:\n",
    "### A. Find patterns between customer purchasing behaviors to identify categories of customers. I.e. how many categories of customers are there based on their purchasing rates? We have repeat customers here since people buy other cosmetics based on their past purchases.\n",
    "\n",
    "### B.Visually inspect the customer categories to identify the distinctive categories and their corresponding descriptive features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMexthxXAGoq"
   },
   "source": [
    "### Task 0: Getting familiar with the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RiEtO2vfAGoq"
   },
   "outputs": [],
   "source": [
    "## Importing required Libraries\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins import projector\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pISugJ7TAGot"
   },
   "outputs": [],
   "source": [
    "## Get working directory\n",
    "PATH = os.getcwd()\n",
    "## Path to save the embedding and checkpoints generated\n",
    "#LOG_DIR = PATH + '/project-tensorboard/'\n",
    "#os.mkdir(LOG_DIR)\n",
    "#LOG_DIR = PATH + '/project-tensorboard/log-1'\n",
    "#os.mkdir(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73-ttpfSAGow"
   },
   "outputs": [],
   "source": [
    "PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0_JDZR5AGoy"
   },
   "outputs": [],
   "source": [
    "LOG_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8w8zTPGAGo1"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Cosmetics_data_2019.csv\")\n",
    "\n",
    "np.shape(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSm6dt8RAGo3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "YY=df.loc[:,'Purchase'].values\n",
    "print(f'Percentage of interactions resulting in a purchase = {(np.sum(YY)/len(YY))*100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP7Y8J7dm6lW"
   },
   "source": [
    "### Exercise: Set the target vector y equal to the `'Purchase'` column of the DataFrame and the features array X equal to the remaining columns, minus `'user_id'` and `'product_id'`. Drop those two columns from the original DataFrame as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hmxyZxpAGpB"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "X  = df.iloc[:,2:-1].values\n",
    "y  = df.iloc[:,-1].values\n",
    "df = df.drop(columns=['user_id','product_id'])\n",
    "### END CODE HERE ###\n",
    "# Display the first few rows of the modified DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z4Sly2EoSol"
   },
   "source": [
    "### Use a library of functions (`helper_functions.py`) to carry out essential tasks such as feature selection\n",
    "\n",
    "### Using a separate library of functions increases modularity of the code without the need to copy and paste code to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxpPZNjLAGpD"
   },
   "outputs": [],
   "source": [
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ATqe8cyGAGpG"
   },
   "source": [
    "## Task 1: Select top features to separate purchasing vs non purchasing customers.\n",
    "### You can modify the functions in helper_functions to include OLS and other feature selection mentods.\n",
    "### Remember to reload the kernel (restart jupyternotebook) if you make any change to helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GblAckfUoyn4"
   },
   "source": [
    "### Exercise: Calculate, print, and plot the feature importances, in descending order of importance. Call the appropriate function from helper_functions.py here.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G0EeGGnWAGpM"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "indices, importances = return_feature_rank_from_RF(X,y)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8-dT3myqtuC"
   },
   "source": [
    "### Exercise: Select features with weight > 0.025 \n",
    "### 0.025 is a significant cutoff point based on the graph above, hence its choice for this exercise.\n",
    "## Note: Weight cutoff is a relative process for each data set and the threshold (0.025 here) varies across data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eS0XlXXuAGpO"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Extract the indices corresponding to a feature importance > 0.025\n",
    "idx = np.where(importances > 0.025)\n",
    "# Extract the names of the associated feature columns\n",
    "selected_columns = df.columns[idx]\n",
    "### END CODE HERE ###\n",
    "print(idx)\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NL_uBA1lud4Q"
   },
   "source": [
    "### Exercise: Use the selected columns as your training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yA7iHYQNAGpR"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "X_train = df[selected_columns].values\n",
    "y_train = y\n",
    "### END CODE HERE ###\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qRuVLj0lAGpT"
   },
   "source": [
    "### Task 2: Apply dimensionality reduction (to speed up) and clustering. Analyze optimal number of clusters using PCA vs. t-SNE. But complete this exercise using PCA only.\n",
    "### t-SNE is very slow, so verify performance (#clusters) for 10k samples using PCA and t-SNE\n",
    "\n",
    "#Question: Do you need to normalize features in range [0,1] prior to PCA or t-SNE? When would normalization help and when would it not help?\n",
    "## Submit your response below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6s3OI6rvTek"
   },
   "source": [
    "### [OPTION 1]: apply PCA followed by Elbow method and Yellowbrick clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WIG5CclFvlLO"
   },
   "source": [
    "### Exercise: Compute a reduced features dataset with 5 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Y9DVVG9AGpU"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "pca = PCA(n_components=5)\n",
    "X_red = pca.fit_transform(X_train)\n",
    "### END CODE HERE ###\n",
    "print(X_red.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1tZlaznwpSR"
   },
   "source": [
    "### Exercise: Apply $k$-means clustering with $k \\in [1, 20]$ to the reduced features data. Visualize the associated distortion scores and computational times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gsFpQiEkAGpX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "### START CODE HERE ###\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(1,20))\n",
    "# Fit the visualizer to the reduced features data\n",
    "visualizer.fit(X_red)\n",
    "### END CODE HERE ###\n",
    "#visualizer.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MftWw10t0qYs"
   },
   "source": [
    "### [OPTION 2]: Apply t-SNE followed by Elbow method and Yellowbrick clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "njBJqFM75qd0"
   },
   "source": [
    "### Exercise: Assume 3 components to be extracted and perplexity of 2. This can take a long time. Use first 10k samples to speed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IuGJg2UPAGpZ"
   },
   "outputs": [],
   "source": [
    "import sklearn.manifold\n",
    "### START CODE HERE ###\n",
    "tsne_op = sklearn.manifold.TSNE(n_components=3, perplexity=2)\n",
    "X_red_t = np.array(tsne_op.fit_transform(X_train[0:10000,:]))\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_B-RrbV-53tJ"
   },
   "source": [
    "### Exercise: Apply $k$-means clustering with $k \\in [1, 20]$ to the t-SNE reduced features data. Visualize the associated distortion scores and computational times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3fHF5-DAGpb"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Instantiate the clustering model and visualizer\n",
    "model = KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(1,20))\n",
    "# Fit the visualizer to the reduced features data\n",
    "visualizer.fit(X_red_t)        # Fit the data to the visualizer\n",
    "### END CODE HERE ###\n",
    "#visualizer.show() \n",
    "# Ideally you should have similar optimal cluster numbers for both PCA and t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IxaAYc863u1"
   },
   "source": [
    "### Exercise: Assign cluster IDs to all the PCA-reduced data samples and store the new data as a csv [before Lunch Break]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kz0uVfFAGpd"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE \n",
    "#Enter the optimal number of clusters here from the Elbow method\n",
    "n = 3 \n",
    "# Instantiate the clustering model\n",
    "clusters = KMeans(n_clusters = n)\n",
    "# Assign the points in the reduced dataset to clusters\n",
    "C_vals = np.array(clusters.fit_predict(X_red))\n",
    "### END CODE HERE ###\n",
    "print(np.unique(C_vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDsrCqos9Gap"
   },
   "source": [
    "### Save the data and associated clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2aDnZi8AGpi"
   },
   "outputs": [],
   "source": [
    "X_save = pd.DataFrame(df[selected_columns])\n",
    "X_save['Purchase'] = df.iloc[:,-1]\n",
    "X_save['ClusterID'] = Ct_vals\n",
    "X_save.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Zg8dtuLAGpn"
   },
   "outputs": [],
   "source": [
    "X_save.to_csv('Clustered_data_cosmetics_tsne.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HB-SZFqUAGpr"
   },
   "source": [
    "### Task 3: Visualize the clusters [Instructor Led]\n",
    "Jupyter notebook + Google colab version https://colab.research.google.com/drive/1-PUVjf4eNO8QzF27lbBg6OT-yTNlBgD7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmsreBzSAGpr"
   },
   "outputs": [],
   "source": [
    "#Load the clustered data and visualize using tensorboard\n",
    "X_train = pd.read_csv(\"Clustered_data_cosmetics.csv\")\n",
    "X=X_train.iloc[:,0:-2].values\n",
    "\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "COq_wrUcAGpt"
   },
   "outputs": [],
   "source": [
    "pca=PCA(n_components=5)\n",
    "X1=pca.fit_transform(X)\n",
    "print(X1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Ro7_dTyAGpw"
   },
   "outputs": [],
   "source": [
    "#version 1\n",
    "def register_embedding(embedding_tensor_name, meta_data_fname, log_dir):\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embedding_tensor_name\n",
    "    embedding.metadata_path = meta_data_fname\n",
    "    projector.visualize_embeddings(log_dir, config)\n",
    "\n",
    "def save_labels_tsv(labels, filepath, log_dir):\n",
    "    with open(os.path.join(log_dir, filepath), 'w') as f:\n",
    "        for label in labels:\n",
    "            f.write('{}\\n'.format(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E_a-gke2AGpy"
   },
   "outputs": [],
   "source": [
    "META_DATA_FNAME = 'meta_cos_reduction1.tsv'  # Labels will be stored here\n",
    "EMBEDDINGS_TENSOR_NAME = 'sample_data_cos_reduction1'\n",
    "EMBEDDINGS_FPATH = os.path.join(LOG_DIR, EMBEDDINGS_TENSOR_NAME + '.ckpt')\n",
    "STEP = 0\n",
    "\n",
    "register_embedding(EMBEDDINGS_TENSOR_NAME, META_DATA_FNAME, LOG_DIR)\n",
    "save_labels_tsv(C_vals, META_DATA_FNAME, LOG_DIR)#version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YziMhdGEAGp1"
   },
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()\n",
    "tensor_embeddings = tf.Variable(X1, name=EMBEDDINGS_TENSOR_NAME)\n",
    "sess = tf.compat.v1.InteractiveSession()\n",
    "sess.run(tf.compat.v1.global_variables_initializer())\n",
    "saver = tf.compat.v1.train.Saver()\n",
    "saver.save(sess, EMBEDDINGS_FPATH, STEP)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-WdbFR1qAGp3"
   },
   "outputs": [],
   "source": [
    "#Attach an image of the optimal clusters and their relative position in t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgmFyvbMAGp5"
   },
   "source": [
    "### Task 4: Analysis of clusters.\n",
    "Now that the records have been clustered, do the following 2 tasks:\n",
    "1. Do the different clusters vary in their purchase ratio? If so by how much?\n",
    "2. Plot the discriminating charecteristics of each feature to separate purchase vs not-purchase events [Hint: Seaborn plot]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oojQC8hz9jwV"
   },
   "source": [
    "### Exercise: Find the unique cluster IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MuCS3KK1AGp6"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Get all the values in the 'ClusterID' column\n",
    "column_values = None\n",
    "# Compute the unique cluster ID values\n",
    "unique_values_a = None\n",
    "# Sort the unique cluster ID values\n",
    "unique_values = None\n",
    "### END CODE HERE ###\n",
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l93Jjag-QSu"
   },
   "source": [
    "### Exercise: For each cluster, calculate the representation percentage (i.e. what percentage of the dataset's samples belong to that cluster) and purchase percentage (i.e. what percentage of samples in each cluster resulted in a purchase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJQDN9KKAGp8"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ###\n",
    "# Initialize the percentages as arrays of zeros, with each zero representing a unique cluster ID\n",
    "purchase_percentage = None\n",
    "representation_percentage = None\n",
    "# Loop through the clusters\n",
    "for index, item in enumerate(unique_values):\n",
    "    locations = None\n",
    "    purchase = None\n",
    "    representation_percentage[index] = None\n",
    "    purchase_percentage[index] = None\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbMCD-TeAGp-"
   },
   "outputs": [],
   "source": [
    "print('Representation Percentage=',(representation_percentage))\n",
    "print('Purchase Percentage=',purchase_percentage)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(representation_percentage,purchase_percentage)\n",
    "plt.xlabel('Representation')\n",
    "plt.ylabel('Purchase Percentage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "npoSZxseAGqA"
   },
   "outputs": [],
   "source": [
    "print(np.sum(purchase_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_k62gA56AGqC"
   },
   "outputs": [],
   "source": [
    "total_purchase=X_train.loc[X_train['Purchase']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kad6lHmEAGqE"
   },
   "outputs": [],
   "source": [
    "purchase_distrib=total_purchase.shape[0]/X_train.shape[0]\n",
    "print(purchase_distrib*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEF0tHT-AGqH"
   },
   "source": [
    "# This implies that cluster 0 represents 94% of the samples and has purchase ratio most similar to the overall purchase ratio. However, there exist smaller clusters with 1.4%, 4.3% population representation that represent higher purchase ratios (customers in those clusters are more sure to buy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_hpTlQFAGqI"
   },
   "outputs": [],
   "source": [
    "# Next, to analyze composition of each cluster\n",
    "#Cluster 0\n",
    "result = X_train.loc[X_train['ClusterID'] == 0]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (20, 25))\n",
    "j = 0\n",
    "for i,item in enumerate(result.columns):\n",
    "    plt.subplot(4, 4, j+1)\n",
    "    j += 1\n",
    "#     if(i==12):\n",
    "#          sb.countplot(x='weekday', hue='Purchase', data=result)\n",
    "#     elif(i==13):\n",
    "#          sb.countplot(x='timeOfDay', hue='Purchase', data=result)\n",
    "#     else:\n",
    "#         item=None\n",
    "    sb.distplot(result[item][result['Purchase']==0], color='b', label = 'No Purchase')\n",
    "    sb.distplot(result[item][result['Purchase']==1], color='r', label = 'Purchase')\n",
    "    plt.legend(loc='best')\n",
    "fig.suptitle('Feature Analysis')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9qPS4kG0AGqK"
   },
   "outputs": [],
   "source": [
    "#Cluster 1\n",
    "result = X_train.loc[X_train['ClusterID'] == 1]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (20, 25))\n",
    "j = 0\n",
    "for i,item in enumerate(result.columns):\n",
    "    plt.subplot(4, 4, j+1)\n",
    "    j += 1\n",
    "\n",
    "    sb.distplot(result[item][result['Purchase']==0], color='b', label = 'No Purchase')\n",
    "    sb.distplot(result[item][result['Purchase']==1], color='r', label = 'Purchase')\n",
    "    plt.legend(loc='best')\n",
    "fig.suptitle('Feature Analysis')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QKF4rVLAGqM"
   },
   "outputs": [],
   "source": [
    "#Cluster 2\n",
    "result = X_train.loc[X_train['ClusterID'] == 2]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (20, 25))\n",
    "j = 0\n",
    "for i,item in enumerate(result.columns):\n",
    "    plt.subplot(4, 4, j+1)\n",
    "    j += 1\n",
    "#     if(i==12):\n",
    "#          sb.countplot(x='weekday', hue='Purchase', data=result)\n",
    "#     elif(i==13):\n",
    "#          sb.countplot(x='timeOfDay', hue='Purchase', data=result)\n",
    "#     else:\n",
    "#         item=None\n",
    "    sb.distplot(result[item][result['Purchase']==0], color='b', label = 'No Purchase')\n",
    "    sb.distplot(result[item][result['Purchase']==1], color='r', label = 'Purchase')\n",
    "    plt.legend(loc='best')\n",
    "fig.suptitle('Feature Analysis')\n",
    "fig.tight_layout()\n",
    "fig.subplots_adjust(top=0.95)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FnySfsUAIZt"
   },
   "source": [
    "### Exercise: Comment on which features are most dissimilar across clusters"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Marketing_Insights_Target_Cosmetics-v2.ipynb",
   "provenance": [
    {
     "file_id": "1SodjnerI5T0RMOvx2s4PtTB5wMdDYREH",
     "timestamp": 1604690525812
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
